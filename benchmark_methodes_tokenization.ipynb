{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Benchmark\n",
    "\n",
    "- Sur la base d'un court texte, ce notebook teste les m√©thodes de tokenisation les plus courantes issues de librairies ou construites √† partir de regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"C'est 1 test pour mieux <h1> tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) ! J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite. Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly. Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Gensim Token Level\n",
      "['C', 'est', 'test', 'pour', 'mieux', 'h', 'tokenizer', 'h', 'les', 'textes', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'depenser', 'J', 'espere', 'que', 'l', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', 'a', 'defaut', 'la', 'meilleure', 'possible', 'et', 'aller', 'fois', 'vite', 'Depuis', 'le', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', 'utilise', 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'Merci', 'fchollet', 'francois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'l', 'excellent', 'ouvrage', 'NLP', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney']\n",
      "\n",
      "\n",
      "Tokens NTLK Word_tokenize\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18/10/2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https', ':', '//towardsdatascience.com/', 'ou', 'des', 'bit.ly', '.', 'Merci', '@', 'fchollet', '(', 'fran√ßois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens NTLK Word_tokenize French Language\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18/10/2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https', ':', '//towardsdatascience.com/', 'ou', 'des', 'bit.ly', '.', 'Merci', '@', 'fchollet', '(', 'fran√ßois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens NTLK Pickle French\n",
      "[\"C'est 1 test pour mieux <h1> tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) !\", \"J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly.\", \"Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\"]\n",
      "\n",
      "\n",
      "Tokens Regex WhitespaceTokenizer\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', '<h1>', 'tokenizer', '<h1>', 'les', 'textes', 'en', '#NLP', 'ou', '#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel)', 'sans', 'd√©penser', '100$', ';-)', '!', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite.', 'Depuis', 'le', '18/10/2021,', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https://towardsdatascience.com/', 'ou', 'des', 'bit.ly.', 'Merci', '@fchollet', '(fran√ßois.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht,', 'Sidharth', 'Ramachandran,', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú.']\n",
      "\n",
      "\n",
      "Tokens NTLK Quatro Sentence Level\n",
      "[\"C'est 1 test pour mieux <h1> tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) !\", \"J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly.\", \"Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\"]\n",
      "\n",
      "\n",
      "Tokens NTLK Quatro Sentence /Token Level\n",
      "[[\"C'est\", '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!'], [\"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.'], ['Depuis', 'le', '18/10/2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https', ':', '//towardsdatascience.com/', 'ou', 'des', 'bit.ly', '.'], ['Merci', '@', 'fchollet', '(', 'fran√ßois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']]\n",
      "\n",
      "\n",
      "Tokens NTLK TweetTokenizer\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', '<h1>', 'tokenizer', '<h1>', 'les', 'textes', 'en', '#NLP', 'ou', '#TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';-)', '!', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18/10', '/', '2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https://towardsdatascience.com/', 'ou', 'des', 'bit.ly', '.', 'Merci', '@fchollet', '(', 'fran√ßois.chollet@google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens NLTK Sent_tokenize\n",
      "[\"C'est 1 test pour mieux <h1> tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) !\", \"J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly.\", \"Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\"]\n",
      "\n",
      "\n",
      "Tokens WordPunctTokenizer\n",
      "['C', \"'\", 'est', '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';-)', '!', 'J', \"'\", 'esp√®re', 'que', 'l', \"'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18', '/', '10', '/', '2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', \"'\", 'utilise', 'https', '://', 'towardsdatascience', '.', 'com', '/', 'ou', 'des', 'bit', '.', 'ly', '.', 'Merci', '@', 'fchollet', '(', 'fran√ßois', '.', 'chollet', '@', 'google', '.', 'com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', \"'\", 'excellent', 'ouvrage', 'NLP', 'pic', '.', 'twitter', '.', 'com', '/', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú.']\n",
      "\n",
      "\n",
      "Tokens NWET Tokenizer\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural_Language_Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18/10/2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https', ':', '//towardsdatascience.com/', 'ou', 'des', 'bit.ly', '.', 'Merci', '@', 'fchollet', '(', 'fran√ßois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep_Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens Spacy Tokenizer Level\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stdbuf was not found; communication with perl may hang due to stdio buffering.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"C'\", 'est', '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', ' ', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';-)', '!', \"J'\", 'esp√®re', 'que', \"l'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18/10/2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'\", 'utilise', 'https://towardsdatascience.com/', 'ou', 'des', 'bit.ly', '.', 'Merci', '@fchollet', '(', 'fran√ßois.chollet@google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'\", 'excellent', 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens Spacy Sentence Level\n",
      "[\"C'est 1 test pour mieux <h1>\", 'tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) !', \"J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly.\", \"Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\"]\n",
      "\n",
      "\n",
      "Tokens Scikit Learn\n",
      "['est', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', 'esp√®re', 'que', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'utilise', 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'Merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'excellent', 'ouvrage', 'NLP', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney']\n",
      "\n",
      "\n",
      "Tokens Regex A\n",
      "['C', \"'est\", '1', 'test', 'pour', 'mieux', '<h1>', 'tokenizer', '<h1>', 'les', 'textes', 'en', '#NLP', 'ou', '#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';-)', '!', 'J', \"'esp√®re\", 'que', 'l', \"'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18', '/10/2021,', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', \"'utilise\", 'https', '://towardsdatascience.com/', 'ou', 'des', 'bit', '.ly.', 'Merci', '@fchollet', '(fran√ßois.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', \"'excellent\", 'ouvrage', 'NLP', 'pic', '.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú.']\n",
      "\n",
      "\n",
      "Tokens Regex B\n",
      "[\"C'\", 'est', '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!', \"J'\", 'esp√®re', 'que', \"l'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18', '/', '10', '/', '2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'\", 'utilise', 'https', ':', '/', '/', 'towardsdatascience', '.', 'com', '/', 'ou', 'des', 'bit', '.', 'ly', '.', 'Merci', '@', 'fchollet', '(', 'fran√ßois', '.', 'chollet', '@', 'google', '.', 'com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'\", 'excellent', 'ouvrage', 'NLP', 'pic', '.', 'twitter', '.', 'com', '/', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens Regex C\n",
      "['est', 'test', 'pour', 'mieux', 'tokenizer', 'les', 'textes', 'NLP', 'TALN', 'Natural', 'Language', 'Processing', 'traitement', 'automatique', 'language', 'naturel', 'sans', 'penser', 'esp', 'que', 'trouvera', 'solution', 'optimale', 'faut', 'meilleure', 'possible', 'aller', 'fois', 'vite', 'Depuis', 'travaille', 'sur', 'les', 'Regex', 'utilise', 'https', 'towardsdatascience', 'com', 'des', 'bit', 'Merci', 'fchollet', 'fran', 'ois', 'chollet', 'google', 'com', 'pour', 'livre', 'sur', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'excellent', 'ouvrage', 'NLP', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney']\n",
      "\n",
      "\n",
      "Tokens Regex D\n",
      "['C', 'est', '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', '!', 'J', 'esp√®re', 'que', 'l', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', 'utilise', 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'Merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'l', 'excellent', 'ouvrage', 'NLP', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney']\n",
      "\n",
      "\n",
      "Tokens Regex E\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', '<h1>', 'tokenizer', '<h1>', 'les', 'textes', 'en', '#NLP', 'ou', '#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel)', 'sans', 'd√©penser', '100$', ';-)', '!', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite.', 'Depuis', 'le', '18/10/2021,', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https://towardsdatascience.com/', 'ou', 'des', 'bit.ly.', 'Merci', '@fchollet', '(fran√ßois.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht,', 'Sidharth', 'Ramachandran,', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú.']\n",
      "\n",
      "\n",
      "Tokens Regex F\n",
      "['C', \"'est\", '1', 'test', 'pour', 'mieux', '<h1>', 'tokenizer', '<h1>', 'les', 'textes', 'en', '#NLP', 'ou', '#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';-)', '!', 'J', \"'esp√®re\", 'que', 'l', \"'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18', '/10/2021,', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', \"'utilise\", 'https', '://towardsdatascience.com/', 'ou', 'des', 'bit', '.ly.', 'Merci', '@fchollet', '(fran√ßois.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', \"'excellent\", 'ouvrage', 'NLP', 'pic', '.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú.']\n",
      "\n",
      "\n",
      "Tokens Regex H\n",
      "['C', 'est', '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', '!', 'J', 'esp√®re', 'que', 'l', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', 'utilise', 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'Merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'l', 'excellent', 'ouvrage', 'NLP', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney']\n",
      "\n",
      "\n",
      "Tokens Regex I\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'Merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney']\n",
      "\n",
      "\n",
      "Tokens Regex J Sentence Level\n",
      "[\"C'est 1 test pour mieux <h1> tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) \", \"J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite\", \"Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly\", \"Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\"]\n",
      "\n",
      "\n",
      "Tokens Regex K\n",
      "[\"C'\", 'est', '1', 'test', 'pour', 'mieux', '<', 'h1', '>', 'tokenizer', '<', 'h1', '>', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!', \"J'\", 'esp√®re', 'que', \"l'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18', '/', '10', '/', '2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'\", 'utilise', 'https', ':', '/', '/', 'towardsdatascience', '.', 'com', '/', 'ou', 'des', 'bit', '.', 'ly', '.', 'Merci', '@', 'fchollet', '(', 'fran√ßois', '.', 'chollet', '@', 'google', '.', 'com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'\", 'excellent', 'ouvrage', 'NLP', 'pic', '.', 'twitter', '.', 'com', '/', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens TextBlob\n",
      "[\"C'est\", '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', \"J'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '18/10/2021', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', \"j'utilise\", 'https', 'towardsdatascience.com', 'ou', 'des', 'bit.ly', 'Merci', 'fchollet', 'fran√ßois.chollet', 'google.com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'NLP', 'pic.twitter.com/abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú']\n",
      "\n",
      "\n",
      "Tokens Keras\n",
      "[\"c'est\", '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'nlp', 'ou', 'taln', 'natural', 'language', 'processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', \"j'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'regex', 'et', \"j'utilise\", 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'deep', 'learning', 'jens', 'albrecht', 'sidharth', 'ramachandran', 'christian', 'winkler', 'pour', \"l'excellent\", 'ouvrage', 'nlp', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'laurence', 'moroney', 'üòú']\n",
      "\n",
      "\n",
      "Tokens Keras + param√®tre longueur tokens\n",
      "[\"c'est\", '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'nlp', 'ou', 'taln', 'natural', 'language', 'processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', \"j'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'regex', 'et', \"j'utilise\", 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'deep', 'learning', 'jens', 'albrecht', 'sidharth', 'ramachandran', 'christian', 'winkler', 'pour', \"l'excellent\", 'ouvrage', 'nlp', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'laurence', 'moroney', 'üòú']\n",
      "\n",
      "\n",
      "Tokens Keras Tensorflow\n",
      "[\"c'est\", '1', 'test', 'pour', 'mieux', 'h1', 'tokenizer', 'h1', 'les', 'textes', 'en', 'nlp', 'ou', 'taln', 'natural', 'language', 'processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'sans', 'd√©penser', '100', \"j'esp√®re\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'depuis', 'le', '18', '10', '2021', 'je', 'travaille', 'sur', 'les', 'regex', 'et', \"j'utilise\", 'https', 'towardsdatascience', 'com', 'ou', 'des', 'bit', 'ly', 'merci', 'fchollet', 'fran√ßois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'deep', 'learning', 'jens', 'albrecht', 'sidharth', 'ramachandran', 'christian', 'winkler', 'pour', \"l'excellent\", 'ouvrage', 'nlp', 'pic', 'twitter', 'com', 'abc', 'sans', 'oublier', 'laurence', 'moroney', 'üòú']\n",
      "\n",
      "\n",
      "Tokens Moses\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "['C', '&apos;est', '1', 'test', 'pour', 'mieux', '&lt;', 'h1', '&gt;', 'tokenizer', '&lt;', 'h1', '&gt;', 'les', 'textes', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', 'sans', 'd√©penser', '100', '$', ';', '-', ')', '!', 'J', '&apos;esp√®re', 'que', 'l', '&apos;on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', '√†', 'd√©faut', 'la', 'meilleure', '&#91;', 'possible', '&#93;', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '18', '/', '10', '/', '2021', ',', 'je', 'travaille', 'sur', 'les', 'Regex', 'et', 'j', '&apos;utilise', 'https', ':', '/', '/', 'towardsdatascience.com', '/', 'ou', 'des', 'bit.ly.', 'Merci', '@', 'fchollet', '(', 'fran√ßois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', '&amp;', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', '&apos;excellent', 'ouvrage', 'NLP', 'pic.twitter.com', '/', 'abc', 'sans', 'oublier', 'Laurence', 'Moroney', 'üòú', '.']\n",
      "\n",
      "\n",
      "Tokens Regex Bert\n",
      " Sentence: C'est 1 test pour mieux <h1> tokenizer <h1> les textes en  #NLP ou #TALN (Natural Language Processing ou traitement automatique du language naturel) sans d√©penser 100$ ;-) ! J'esp√®re que l'on trouvera la solution #optimale ou √† d√©faut la meilleure [possible] et aller 120 fois + vite. Depuis le 18/10/2021, je travaille sur les Regex et j'utilise https://towardsdatascience.com/ ou des bit.ly. Merci @fchollet (fran√ßois.chollet@google.com) pour le livre sur le Deep Learning & Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage NLP pic.twitter.com/abc sans oublier Laurence Moroney üòú.\n",
      "\n",
      "\n",
      "   Tokens: ['‚ñÅC', \"'\", 'est', '‚ñÅ1', '‚ñÅtest', '‚ñÅpour', '‚ñÅmieux', '‚ñÅ<', 'h', '1', '>', '‚ñÅto', 'ken', 'izer', '‚ñÅ<', 'h', '1', '>', '‚ñÅles', '‚ñÅtextes', '‚ñÅen', '‚ñÅ#', 'N', 'LP', '‚ñÅou', '‚ñÅ#', 'TAL', 'N', '‚ñÅ(', 'N', 'a', 'tur', 'al', '‚ñÅLa', 'ngu', 'age', '‚ñÅ', 'Process', 'ing', '‚ñÅou', '‚ñÅtraitement', '‚ñÅautomatique', '‚ñÅdu', '‚ñÅla', 'ngu', 'age', '‚ñÅnaturel', ')', '‚ñÅsans', '‚ñÅd√©penser', '‚ñÅ100', '$', '‚ñÅ;-)', '‚ñÅ!', '‚ñÅJ', \"'\", 'esp√®re', '‚ñÅque', '‚ñÅl', \"'\", 'on', '‚ñÅtrouvera', '‚ñÅla', '‚ñÅsolution', '‚ñÅ#', 'optim', 'ale', '‚ñÅou', '‚ñÅ√†', '‚ñÅd√©faut', '‚ñÅla', '‚ñÅmeilleure', '‚ñÅ[', 'pos', 'sible', ']', '‚ñÅet', '‚ñÅaller', '‚ñÅ120', '‚ñÅfois', '‚ñÅ+', '‚ñÅvite', '.', '‚ñÅDepuis', '‚ñÅle', '‚ñÅ18', '/10/', '20', '21', ',', '‚ñÅje', '‚ñÅtravaille', '‚ñÅsur', '‚ñÅles', '‚ñÅReg', 'ex', '‚ñÅet', '‚ñÅj', \"'\", 'utilise', '‚ñÅhttps', '://', 'to', 'ward', 's', 'data', 'science', '.', 'com', '/', '‚ñÅou', '‚ñÅdes', '‚ñÅ', 'bit', '.', 'ly', '.', '‚ñÅMerci', '‚ñÅ@', 'f', 'ch', 'ollet', '‚ñÅ(', 'fr', 'an', '√ßois', '.', 'ch', 'ollet', '@', 'google', '.', 'com', ')', '‚ñÅpour', '‚ñÅle', '‚ñÅlivre', '‚ñÅsur', '‚ñÅle', '‚ñÅDeep', '‚ñÅLe', 'ar', 'ning', '‚ñÅ&', '‚ñÅJe', 'ns', '‚ñÅAl', 'b', 'recht', ',', '‚ñÅSi', 'dh', 'art', 'h', '‚ñÅRam', 'ach', 'andra', 'n', ',', '‚ñÅChristian', '‚ñÅWin', 'k', 'ler', '‚ñÅpour', '‚ñÅl', \"'\", 'excellent', '‚ñÅouvrage', '‚ñÅN', 'LP', '‚ñÅpic', '.', 'twitter', '.', 'com', '/', 'ab', 'c', '‚ñÅsans', '‚ñÅoublier', '‚ñÅLaurence', '‚ñÅMor', 'on', 'ey', '‚ñÅ', 'üòú', '.']\n",
      "\n",
      "\n",
      "Token IDs: [84, 11, 41, 124, 2006, 24, 334, 2840, 133, 367, 1423, 1200, 6840, 23038, 2840, 133, 367, 1423, 19, 1726, 22, 840, 607, 16368, 47, 840, 27575, 607, 38, 607, 55, 4165, 341, 61, 14112, 491, 21, 28765, 402, 47, 791, 3314, 25, 13, 14112, 491, 1649, 53, 112, 11104, 779, 3057, 8473, 83, 121, 11, 1612, 27, 17, 11, 88, 8248, 13, 932, 840, 15813, 1387, 47, 15, 2193, 13, 1058, 403, 7138, 6997, 374, 14, 632, 4156, 151, 597, 715, 9, 795, 16, 301, 14270, 1461, 2139, 7, 50, 2023, 32, 19, 14493, 850, 14, 76, 11, 4123, 3047, 657, 891, 10004, 10, 12818, 17509, 9, 310, 122, 47, 20, 21, 3981, 9, 1107, 9, 757, 1862, 362, 751, 23371, 38, 427, 364, 14292, 9, 751, 23371, 2266, 29446, 9, 310, 53, 24, 16, 510, 32, 16, 25291, 54, 848, 5141, 537, 100, 2306, 1067, 442, 27862, 7, 168, 13818, 559, 133, 5545, 3103, 21000, 255, 7, 4012, 8378, 496, 1226, 24, 17, 11, 17050, 2961, 278, 16368, 5461, 9, 21353, 9, 310, 122, 2021, 216, 112, 2078, 16175, 3207, 88, 2842, 21, 3, 9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#r'\\w+(?:\\'\\w+){0,2}|[^\\w\\s]'\n",
    "print (\"Tokens Gensim Token Level\")\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "gensim_tokens = list (tokenize(sample_text, deacc = True))\n",
    "print(gensim_tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens NTLK Word_tokenize\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens NTLK Word_tokenize French Language\")\n",
    "tokens = word_tokenize(sample_text, language='french')\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens NTLK Pickle French\")\n",
    "import nltk.data\n",
    "#chargement du tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens Regex WhitespaceTokenizer\")\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer=WhitespaceTokenizer()\n",
    "print(tokenizer.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print (\"Tokens NTLK Quatro Sentence Level\")\n",
    "print(sent_tokenize(sample_text,language='french'))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens NTLK Quatro Sentence /Token Level\")\n",
    "print([word_tokenize(t, language='french') for t in sent_tokenize(sample_text)])\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens NTLK TweetTokenizer\")\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokenizer = tweet_tokenizer\n",
    "print(tokenizer.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens NLTK Sent_tokenize\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenizer = sent_tokenize(sample_text)\n",
    "print(tokenizer)\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens WordPunctTokenizer\") #OK\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens NWET Tokenizer\")\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Deep', 'Learning'))\n",
    "tokenizer.add_mwe(('Natural', 'Language', 'Processing'))\n",
    "tokenizer.add_mwe(('traitement', 'automatique', \"du', 'language\"))\n",
    "print(tokenizer.tokenize(word_tokenize(sample_text)))\n",
    "print(\"\\n\")\n",
    "\n",
    "import spacy\n",
    "print (\"Tokens Spacy Tokenizer Level\")\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "doc = nlp_fr(sample_text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Spacy Sentence Level\")\n",
    "nlp_fr.add_pipe('sentencizer')\n",
    "doc = nlp_fr(sample_text)\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)\n",
    "print(\"\\n\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "print (\"Tokens Scikit Learn\")\n",
    "tokenizer_scikit = RegexpTokenizer(r'(?u)\\b\\w\\w+\\b')\n",
    "print(tokenizer_scikit.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "import re\n",
    "print (\"Tokens Regex A\")\n",
    "tokenizer_a = RegexpTokenizer('(\\w+|\\$[\\d\\.]+|\\S+)')\n",
    "print(tokenizer_a.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex B\")\n",
    "tokenizer_b = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "print(tokenizer_b.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex C\")\n",
    "tokenizer_c = RegexpTokenizer('[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "print(tokenizer_c.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex D\")\n",
    "tokenizer_d = RegexpTokenizer('(\\w+|#\\d|\\?|!)')\n",
    "print(tokenizer_d.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex E\")\n",
    "espace = r\"\\s+\"\n",
    "print(re.split(espace, sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens Regex F\")\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern ='\\w+|\\$[\\d\\.]+|\\S+'\n",
    "print(regexp_tokenize(sample_text, pattern))\n",
    "print(\"\\n\")\n",
    "            \n",
    "print (\"Tokens Regex H\")\n",
    "tokenizer = RegexpTokenizer( r\"(\\w+|#\\d|\\?|!)\")\n",
    "print(tokenizer.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex I\")\n",
    "print(re.findall(\"[\\w']+\",sample_text))\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex J Sentence Level\")\n",
    "sentences = re.compile('[.!?] ').split(sample_text)\n",
    "print(sentences)\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Regex K\")\n",
    "tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "print(tokenizer.tokenize(sample_text))\n",
    "print(\"\\n\")\n",
    "         \n",
    "print (\"Tokens TextBlob\")\n",
    "from textblob import TextBlob\n",
    "print(TextBlob(sample_text).words)\n",
    "print(\"\\n\")\n",
    "      \n",
    "print (\"Tokens Keras\")\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens Keras + param√®tre longueur tokens\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "ntoken = Tokenizer(len(sample_text))\n",
    "ntoken.fit_on_texts(sample_text)\n",
    "print(text_to_word_sequence(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens Keras Tensorflow\")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokens = text_to_word_sequence(sample_text)\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens Moses\")\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "tokenizer  = MosesTokenizer()\n",
    "print(tokenizer(sample_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "print (\"Tokens Regex Bert\")\n",
    "from tensorflow import keras \n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertConfig,BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\")\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f' Sentence: {sample_text}')\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f'Token IDs: {token_ids}')\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

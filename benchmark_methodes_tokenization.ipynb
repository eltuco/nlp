{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Benchmark (non exhaustif) des méthodes de tokenisation <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/> Ce notebook teste les méthodes de tokenisation les plus courantes issues de librairies ou construites à partir de regex.<hr/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères \\\n",
    "en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). \\\n",
    "J'espère que l'on trouvera la solution #optimale ou à défaut \\\n",
    "la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex \\\n",
    "sur les regex mais également les fonctionnalités des librairies de nlp en python. \\\n",
    "En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, \\\n",
    "Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney \\\n",
    "Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning \\\n",
    "https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Gensim Token Level\u001b[0m\n",
      "['Ce', 'texte', 'sert', 'a', 'tester', 'differentes', 'methodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caracteres', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'J', 'espere', 'que', 'l', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', 'a', 'defaut', 'la', 'meilleure', 'possible', 'et', 'aller', 'fois', 'vite', 'Depuis', 'le', 'j', 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'egalement', 'les', 'fonctionnalites', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'l', 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', 'merci', 'a', 'fchollet', 'francois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'tres', 'complet', 'et', 'pedagogique']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print ('\\033[1mTokens Gensim Token Level\\033[0m')\n",
    "from gensim.utils import tokenize\n",
    "gensim_tokens = list (tokenize(sample_text, deacc = True))\n",
    "print(gensim_tokens)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[Tokens NTLK Word_tokenize\u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01/01/2021', ',', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', ':', '//www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[Tokens NTLK Word_tokenize\\033[0m\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NTLK Word_tokenize French Language \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01/01/2021', ',', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', ':', '//www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens NTLK Word_tokenize French Language \\033[0m\")\n",
    "tokens = word_tokenize(sample_text, language='french')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NTLK Pickle French \u001b[0m\n",
      "['Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel).', \"J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python.\", \"En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens NTLK Pickle French \\033[0m\")\n",
    "import nltk.data\n",
    "#chargement du tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex WhitespaceTokenizer\u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#NLP', 'ou#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel).', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite.', 'Depuis', 'le', '01/01/2021,', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date,', 'le', 'livre', 'de', 'Jens', 'Albrecht,', 'Sidharth', 'Ramachandran,', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin,', 'merci', 'à', '@fchollet', 'françois.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https://www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex WhitespaceTokenizer\\033[0m\")\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer=WhitespaceTokenizer()\n",
    "print(tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NTLK Quatro Sentence Level \u001b[0m\n",
      "['Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel).', \"J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python.\", \"En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print (\"\\033[1mTokens NTLK Quatro Sentence Level \\033[0m\")\n",
    "print(sent_tokenize(sample_text,language='french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NTLK Quatro Sentence /Token Level \u001b[0m\n",
      "[['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.'], [\"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.'], ['Depuis', 'le', '01/01/2021', ',', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.'], ['En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', ':', '//www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens NTLK Quatro Sentence /Token Level \\033[0m\")\n",
    "print([word_tokenize(t, language='french') for t in sent_tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NTLK TweetTokenizer \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#NLP', 'ou', '#TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01/01', '/', '2021', ',', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@fchollet', 'françois.chollet@google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https://www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '...']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens NTLK TweetTokenizer \\033[0m\")\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokenizer = tweet_tokenizer\n",
    "print(tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NLTK Sent_tokenize \u001b[0m\n",
      "['Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel).', \"J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python.\", \"En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens NLTK Sent_tokenize \\033[0m\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenizer = sent_tokenize(sample_text)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens WordPunctTokenizer \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ').', 'J', \"'\", 'espère', 'que', 'l', \"'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01', '/', '01', '/', '2021', ',', 'j', \"'\", 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', \"'\", 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois', '.', 'chollet', '@', 'google', '.', 'com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', '://', 'www', '.', 'manning', '.', 'com', '/', 'books', '/', 'deep', '-', 'learning', '-', 'with', '-', 'python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens WordPunctTokenizer \\033[0m\")  #best\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens NWET Tokenizer \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural_Language_Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01/01/2021', ',', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep_Learning', 'https', ':', '//www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens NWET Tokenizer \\033[0m\")\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Deep', 'Learning'))\n",
    "tokenizer.add_mwe(('Natural', 'Language', 'Processing'))\n",
    "tokenizer.add_mwe(('traitement', 'automatique', \"du', 'language\"))\n",
    "print(tokenizer.tokenize(word_tokenize(sample_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Scikit Learn \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'espère', 'que', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '01', '01', '2021', 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', 'merci', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "print (\"\\033[1mTokens Scikit Learn \\033[0m\")\n",
    "tokenizer_scikit = RegexpTokenizer(r'(?u)\\b\\w\\w+\\b')\n",
    "print(tokenizer_scikit.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Spacy Tokenizer Level \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou#TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'\", 'espère', 'que', \"l'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01/01/2021', ',', \"j'\", 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'\", 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@fchollet', 'françois.chollet@google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https://www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "import spacy\n",
    "print (\"\\033[1mTokens Spacy Tokenizer Level \\033[0m\")\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "doc = nlp_fr(sample_text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Spacy Sentence Level \u001b[0m\n",
      "['Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel).', \"J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite.\", \"Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python.\", \"En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "nlp_fr.add_pipe('sentencizer')\n",
    "doc = nlp_fr(sample_text)\n",
    "print (\"\\033[1mTokens Spacy Sentence Level\\033[0m\")\n",
    "\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex A \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#NLP', 'ou', '#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ').', 'J', \"'espère\", 'que', 'l', \"'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01', '/01/2021,', 'j', \"'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', \"'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@fchollet', 'françois', '.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', '://www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "import re\n",
    "print (\"\\033[1mTokens Regex A \\033[0m\")\n",
    "tokenizer_a = RegexpTokenizer('(\\w+|\\$[\\d\\.]+|\\S+)')\n",
    "print(tokenizer_a.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex B \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'\", 'espère', 'que', \"l'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01', '/', '01', '/', '2021', ',', \"j'\", 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'\", 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois', '.', 'chollet', '@', 'google', '.', 'com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', ':', '/', '/', 'www', '.', 'manning', '.', 'com', '/', 'books', '/', 'deep', '-', 'learning', '-', 'with', '-', 'python', 'très', 'complet', 'et', 'pédagogique', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex B \\033[0m\")\n",
    "tokenizer_b = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "print(tokenizer_b.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex C \u001b[0m\n",
      "['texte', 'sert', 'tester', 'diff', 'rentes', 'thodes', 'tokenisation', 'chaines', 'caract', 'res', 'NLP', 'TALN', 'Natural', 'Language', 'Processing', 'traitement', 'automatique', 'language', 'naturel', 'esp', 'que', 'trouvera', 'solution', 'optimale', 'faut', 'meilleure', 'possible', 'aller', 'fois', 'vite', 'Depuis', 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'galement', 'les', 'fonctionnalit', 'des', 'librairies', 'nlp', 'python', 'tant', 'que', 'practicien', 'date', 'livre', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'excellent', 'ouvrage', 'sur', 'NLP', 'sans', 'oublier', 'celui', 'Laurence', 'Moroney', 'Enfin', 'merci', 'fchollet', 'fran', 'ois', 'chollet', 'google', 'com', 'pour', 'livre', 'sur', 'Deep', 'Learning', 'https', 'www', 'manning', 'com', 'books', 'deep-learning-with-python', 'complet', 'dagogique']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex C \\033[0m\")\n",
    "tokenizer_c = RegexpTokenizer('[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "print(tokenizer_c.tokenize(sample_text))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex D \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'J', 'espère', 'que', 'l', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '01', '01', '2021', 'j', 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'l', 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', 'merci', 'à', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex D \\033[0m\")\n",
    "tokenizer_d = RegexpTokenizer('(\\w+|#\\d|\\?|!)')\n",
    "print(tokenizer_d.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex E \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#NLP', 'ou#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel).', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite.', 'Depuis', 'le', '01/01/2021,', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date,', 'le', 'livre', 'de', 'Jens', 'Albrecht,', 'Sidharth', 'Ramachandran,', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin,', 'merci', 'à', '@fchollet', 'françois.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https://www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex E \\033[0m\")\n",
    "espace = r\"\\s+\"\n",
    "print(re.split(espace, sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex F \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#NLP', 'ou', '#TALN', '(Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ').', 'J', \"'espère\", 'que', 'l', \"'on\", 'trouvera', 'la', 'solution', '#optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[possible]', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01', '/01/2021,', 'j', \"'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', \"'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@fchollet', 'françois', '.chollet@google.com)', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', '://www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique', '....']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex F \\033[0m\")\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern ='\\w+|\\$[\\d\\.]+|\\S+'\n",
    "print(regexp_tokenize(sample_text, pattern))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex H \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', 'J', 'espère', 'que', 'l', 'on', 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '01', '01', '2021', 'j', 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', 'l', 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', 'merci', 'à', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex H \\033[0m\")\n",
    "tokenizer = RegexpTokenizer( r\"(\\w+|#\\d|\\?|!)\")\n",
    "print(tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex I \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '01', '01', '2021', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', 'merci', 'à', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex I \\033[0m\")\n",
    "print(re.findall(\"[\\w']+\",sample_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex J Sentence Level \u001b[0m\n",
      "['Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel)', \"J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite\", \"Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python\", \"En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex J Sentence Level \\033[0m\")\n",
    "sentences = re.compile('[.!?] ').split(sample_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex K \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', \"J'\", 'espère', 'que', \"l'\", 'on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '[', 'possible', ']', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01', '/', '01', '/', '2021', ',', \"j'\", 'utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', \"l'\", 'excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois', '.', 'chollet', '@', 'google', '.', 'com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', ':', '/', '/', 'www', '.', 'manning', '.', 'com', '/', 'books', '/', 'deep', '-', 'learning', '-', 'with', '-', 'python', 'très', 'complet', 'et', 'pédagogique', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex K \\033[0m\")\n",
    "tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "print(tokenizer.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens TextBlob \u001b[0m\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'NLP', 'ou', 'TALN', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', \"J'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'Depuis', 'le', '01/01/2021', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'Jens', 'Albrecht', 'Sidharth', 'Ramachandran', 'Christian', 'Winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', 'merci', 'à', 'fchollet', 'françois.chollet', 'google.com', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', 'www.manning.com/books/deep-learning-with-python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens TextBlob \\033[0m\")\n",
    "from textblob import TextBlob\n",
    "print(TextBlob(sample_text).words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Keras \u001b[0m\n",
      "['ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'nlp', 'ou', 'taln', 'natural', 'language', 'processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', \"j'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'depuis', 'le', '01', '01', '2021', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'en', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'jens', 'albrecht', 'sidharth', 'ramachandran', 'christian', 'winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'nlp', 'sans', 'oublier', 'celui', 'de', 'laurence', 'moroney', 'enfin', 'merci', 'à', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'deep', 'learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "\n",
    "print('\\n')\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "print (\"\\033[1mTokens Keras \\033[0m\")\n",
    "\n",
    "print(text_to_word_sequence(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Keras + paramètre longueur tokens \u001b[0m\n",
      "['ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'nlp', 'ou', 'taln', 'natural', 'language', 'processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', \"j'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'depuis', 'le', '01', '01', '2021', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'en', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'jens', 'albrecht', 'sidharth', 'ramachandran', 'christian', 'winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'nlp', 'sans', 'oublier', 'celui', 'de', 'laurence', 'moroney', 'enfin', 'merci', 'à', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'deep', 'learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "print (\"\\033[1mTokens Keras + paramètre longueur tokens \\033[0m\")\n",
    "\n",
    "ntoken = Tokenizer(len(sample_text))\n",
    "ntoken.fit_on_texts(sample_text)\n",
    "print(text_to_word_sequence(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Keras Tensorflow \u001b[0m\n",
      "['ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', 'nlp', 'ou', 'taln', 'natural', 'language', 'processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', \"j'espère\", 'que', \"l'on\", 'trouvera', 'la', 'solution', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', 'possible', 'et', 'aller', '120', 'fois', 'vite', 'depuis', 'le', '01', '01', '2021', \"j'utilise\", 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', 'en', 'tant', 'que', 'practicien', 'de', 'la', 'date', 'le', 'livre', 'de', 'jens', 'albrecht', 'sidharth', 'ramachandran', 'christian', 'winkler', 'pour', \"l'excellent\", 'ouvrage', 'sur', 'le', 'nlp', 'sans', 'oublier', 'celui', 'de', 'laurence', 'moroney', 'enfin', 'merci', 'à', 'fchollet', 'françois', 'chollet', 'google', 'com', 'pour', 'le', 'livre', 'sur', 'le', 'deep', 'learning', 'https', 'www', 'manning', 'com', 'books', 'deep', 'learning', 'with', 'python', 'très', 'complet', 'et', 'pédagogique']\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Keras Tensorflow \\033[0m\")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokens = text_to_word_sequence(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Regex Bert\u001b[0m\n",
      " Sentence: Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "   Tokens: ['▁Ce', '▁texte', '▁sert', '▁à', '▁tester', '▁différentes', '▁méthodes', '▁de', '▁to', 'ken', 'isation', '▁de', '▁chaines', '▁de', '▁caractères', '▁en', '▁#', 'N', 'LP', '▁ou', '#', 'TAL', 'N', '▁(', 'N', 'a', 'tur', 'al', '▁La', 'ngu', 'age', '▁', 'Process', 'ing', '▁ou', '▁traitement', '▁automatique', '▁du', '▁la', 'ngu', 'age', '▁naturel', ').', '▁J', \"'\", 'espère', '▁que', '▁l', \"'\", 'on', '▁trouvera', '▁la', '▁solution', '▁#', 'optim', 'ale', '▁ou', '▁à', '▁défaut', '▁la', '▁meilleure', '▁[', 'pos', 'sible', ']', '▁et', '▁aller', '▁120', '▁fois', '▁+', '▁vite', '.', '▁Depuis', '▁le', '▁01', '/01', '/20', '21', ',', '▁j', \"'\", 'utilise', '▁les', '▁reg', 'ex', '▁sur', '▁les', '▁reg', 'ex', '▁mais', '▁également', '▁les', '▁fonctionnalités', '▁des', '▁librairie', 's', '▁de', '▁n', 'l', 'p', '▁en', '▁p', 'yth', 'on', '.', '▁En', '▁tant', '▁que', '▁p', 'rac', 'ticien', '▁de', '▁la', '▁date', ',', '▁le', '▁livre', '▁de', '▁Je', 'ns', '▁Al', 'b', 'recht', ',', '▁Si', 'dh', 'art', 'h', '▁Ram', 'ach', 'andra', 'n', ',', '▁Christian', '▁Win', 'k', 'ler', '▁pour', '▁l', \"'\", 'excellent', '▁ouvrage', '▁sur', '▁le', '▁N', 'LP', '▁sans', '▁oublier', '▁celui', '▁de', '▁Laurence', '▁Mor', 'on', 'ey', '▁Enfin', ',', '▁merci', '▁à', '▁@', 'f', 'ch', 'ollet', '▁fran', 'çois', '.', 'ch', 'ollet', '@', 'google', '.', 'com', ')', '▁pour', '▁le', '▁livre', '▁sur', '▁le', '▁Deep', '▁Le', 'ar', 'ning', '▁https', '://', 'www', '.', 'mann', 'ing', '.', 'com', '/', 'book', 's', '/', 'de', 'ep', '-', 'learning', '-', 'w', 'ith', '-', 'p', 'yth', 'on', '▁très', '▁complet', '▁et', '▁pédagogique', '.', '...']\n",
      "\n",
      "\n",
      "Token IDs: [148, 930, 2090, 15, 3019, 678, 2849, 8, 1200, 6840, 1385, 8, 20313, 8, 7370, 22, 840, 607, 16368, 47, 5445, 27575, 607, 38, 607, 55, 4165, 341, 61, 14112, 491, 21, 28765, 402, 47, 791, 3314, 25, 13, 14112, 491, 1649, 120, 121, 11, 1612, 27, 17, 11, 88, 8248, 13, 932, 840, 15813, 1387, 47, 15, 2193, 13, 1058, 403, 7138, 6997, 374, 14, 632, 4156, 151, 597, 715, 9, 795, 16, 3455, 10457, 14986, 2139, 7, 76, 11, 4123, 19, 16333, 850, 32, 19, 16333, 850, 65, 200, 19, 4241, 20, 8114, 10, 8, 49, 219, 286, 22, 387, 17698, 88, 9, 107, 376, 27, 387, 4192, 9305, 8, 13, 749, 7, 16, 510, 8, 100, 2306, 1067, 442, 27862, 7, 168, 13818, 559, 133, 5545, 3103, 21000, 255, 7, 4012, 8378, 496, 1226, 24, 17, 11, 17050, 2961, 32, 16, 278, 16368, 112, 2078, 330, 8, 16175, 3207, 88, 2842, 1078, 7, 895, 15, 1862, 362, 751, 23371, 9871, 14292, 9, 751, 23371, 2266, 29446, 9, 310, 53, 24, 16, 510, 32, 16, 25291, 54, 848, 5141, 3047, 657, 1253, 9, 4875, 402, 9, 310, 122, 6780, 10, 122, 234, 3737, 26, 24943, 26, 640, 7395, 26, 286, 17698, 88, 95, 1183, 14, 5362, 9, 57]\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Regex Bert\\033[0m\")\n",
    "from tensorflow import keras \n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertConfig,BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\")\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f' Sentence: {sample_text}')\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stdbuf was not found; communication with perl may hang due to stdio buffering.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original ---- \n",
      " Ce texte sert à tester différentes méthodes de tokenisation de chaines de caractères en #NLP ou#TALN (Natural Language Processing ou traitement automatique du language naturel). J'espère que l'on trouvera la solution #optimale ou à défaut la meilleure [possible] et aller 120 fois + vite. Depuis le 01/01/2021, j'utilise les regex sur les regex mais également les fonctionnalités des librairies de nlp en python. En tant que practicien de la date, le livre de Jens Albrecht, Sidharth Ramachandran, Christian Winkler pour l'excellent ouvrage sur le NLP sans oublier celui de Laurence Moroney Enfin, merci à @fchollet françois.chollet@google.com) pour le livre sur le Deep Learning https://www.manning.com/books/deep-learning-with-python très complet et pédagogique....\n",
      "\n",
      "\n",
      "\u001b[1mTokens Moses\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "['Ce', 'texte', 'sert', 'à', 'tester', 'différentes', 'méthodes', 'de', 'tokenisation', 'de', 'chaines', 'de', 'caractères', 'en', '#', 'NLP', 'ou', '#', 'TALN', '(', 'Natural', 'Language', 'Processing', 'ou', 'traitement', 'automatique', 'du', 'language', 'naturel', ')', '.', 'J', '&apos;espère', 'que', 'l', '&apos;on', 'trouvera', 'la', 'solution', '#', 'optimale', 'ou', 'à', 'défaut', 'la', 'meilleure', '&#91;', 'possible', '&#93;', 'et', 'aller', '120', 'fois', '+', 'vite', '.', 'Depuis', 'le', '01', '/', '01', '/', '2021', ',', 'j', '&apos;utilise', 'les', 'regex', 'sur', 'les', 'regex', 'mais', 'également', 'les', 'fonctionnalités', 'des', 'librairies', 'de', 'nlp', 'en', 'python', '.', 'En', 'tant', 'que', 'practicien', 'de', 'la', 'date', ',', 'le', 'livre', 'de', 'Jens', 'Albrecht', ',', 'Sidharth', 'Ramachandran', ',', 'Christian', 'Winkler', 'pour', 'l', '&apos;excellent', 'ouvrage', 'sur', 'le', 'NLP', 'sans', 'oublier', 'celui', 'de', 'Laurence', 'Moroney', 'Enfin', ',', 'merci', 'à', '@', 'fchollet', 'françois.chollet', '@', 'google.com', ')', 'pour', 'le', 'livre', 'sur', 'le', 'Deep', 'Learning', 'https', ':', '/', '/', 'www.manning.com', '/', 'books', '/', 'deep', '@-@', 'learning', '@-@', 'with', '@-@', 'python', 'très', 'complet', 'et', 'pédagogique', '....']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Texte original ---- \\n {sample_text}')\n",
    "print('\\n')\n",
    "\n",
    "print (\"\\033[1mTokens Moses\\033[0m\")\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "tokenizer  = MosesTokenizer()\n",
    "print(tokenizer(sample_text))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
